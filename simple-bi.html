<!DOCTYPE HTML>

<html>

<head>
	<title>Deep Learning for Network Traffic Prediction</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>

<body class="is-preload">

	<!-- Simple LSTM -->
	<section id="simple" class="main style1">
		<div class="container">
			<div class="inner">
				<ul class="actions special">
					<li><a href="index.html" class="button scrolly">Home</a></li>
					<li><a href="#simple" class="button scrolly">Simple LSTM</a></li>
					<li><a href="#bi" class="button scrolly">Bidirectional LSTM</a></li>
					<li><a href="#hyper" class="button scrolly">Hyperparameters</a></li>
				</ul>
			</div>
			<div class="row gtr-150">
				<div class="col-6 col-12-medium">
					<header class="major">
						<h2>Baseline LSTM<br />
						</h2>
					</header>
					<h3>The Simple Child</h3>
					<p>
						An LSTM is a type of Recurrent Neural Network, which is formed by <strong> adding a short and long term
						memory unit </strong> to an RNN.
						The addition of memory units allows the network to deal with the correlation of time series in
						the short and long term,
						and <strong> store dependencies that it deems important </strong> from earlier epochs of training.
					</p>
					<p>
						LSTMs have cells in the hidden layers of the neural network, which have three gates: input, an
						output, and a forget gate.
						These gates control the flow of information which is needed to predict the output in the
						network.
						The gates that are added to an LSTM cell allow LSTMs to learn long term dependencies, since they
						are able to retain information from multiple previous time-steps.
					</p>
					<p>
						The image on the right is an example of an LSTM cell.
						Input is received from the previous cell and passes through the forget gates, where values
						deemed unimportant are dropped.
						Eventually new values are sent to the next cell through the output gate.
					</p>
					<ul class="actions special">
							<li><a href="#bi" class="button scrolly">Next: Bidirectional LSTM</a></li>
						</ul>
				</div>
				<div class="col-6 col-12-medium imp-medium">
					<span class="image fit"><img src="./images/RHNrZ.jpeg" alt="" /></span>
				</div>
			</div>
	</section>

	<!-- Preliminary Data Analysis -->
	<section id="bi" class="main style2">
		<div class="container">
			<div class="row gtr-150">
				<div class="col-6 col-12-medium imp-medium">
					<span class="image fit"><img src="./images/bi.jpeg" alt="" /></span>
					<span class="image fit"><img src="./images/bi-table.png" alt="" /></span>
				</div>
				<div class="col-6 col-12-medium">
					<header class="major">
						<h2>Bidirectional LSTM<br />
						</h2>
					</header>
					<h3>Going Back and Forth</h3>
					<p>
						A Bidirectional LSTM runs input from both past to future, and future to past.
						This approach <strong> preserves information from the future </strong> and, using two hidden states combined,
						it is is able in any point in time to preserve information.
					</p>
					<p>
						The Bidirectional LSTM fitted to the training data very well, and had the lowest validation loss
						of all the LSTM models.
						Since information is considered from both the past and the future, it may have considered future
						flows which the other models were not aware of yet.
						During the prediction phase, the Bidirectional LSTM had a higher error rate than the Simple and
						Stacked LSTM which led us to believe it was overcompensating for the fluctuations in traffic we saw later on.
					</p>
					<p>The results show that there is no benefit to using the Bidirectional model for training and
						predicting network traffic on the SANREN,
						as its complexity does not yield an improvement in prediction accuracy compared to the Simple
						LSTM.</p>
						<ul class="actions special">
								<li><a href="#hyper" class="button scrolly">Next: Hyperparameter Tuning</a></li>
							</ul>
				</div>
			</div>
		</div>
	</section>

	<!-- Stacked LSTM -->
	<section id="hyper" class="main style1">
		<div class="container">
			<div class="row gtr-150">
				<div class="col-6 col-12-medium imp-medium">
					<span class="image fit"><img src="./images/SimpleMSEvsEpochs.png" alt="" /></span>
					<span class="image fit"><img src="./images/TrainingvsEpochs.png" alt="" /></span>
				</div>
				<div class="col-6 col-12-medium">
				
					<h2>Hyperparameter Tuning</h2>
						<header class="major">
					</header>
					<h3>It's A Fine Line</h3>
					<p>
						Training and validation loss allowed for the optimal hyperparamters to be chosen.
						When validation loss reached a minimum, training was stopped. Including more neurons, or the number of memory units
						in each layer proved to be advantageous. However, there is no such thing as a free lunch. Training time too increased, as more neurons were added.
						Both the Simple and Bidirectional LSTMs maintained a consistently lower training time than the Stacked LSTM. They are less complex than the Stacked LSTM,
					using less layers too. 
					</p>
					<ul class="actions special">
						<li><a href="index.html#models" class="button scrolly">Back to Home</a></li>
					</ul>
				</div>
			</div>
		</div>
	</section>

	<!-- Footer -->
	<section id="footer">
		<ul class="icons">
			<li><a href="https://github.com/fle1scha/Deep-Learning-Network-Traffic-Prediction"
					class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
		</ul>
		<ul class="copyright">
			<li>&copy; Justin Myerson and Antony Fleischer</li>
		</ul>
	</section>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>
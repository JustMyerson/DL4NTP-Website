<!DOCTYPE HTML>

<html>

<head>
	<title>Deep Learning for Network Traffic Prediction</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>

<body class="is-preload">

	<!-- Simple LSTM -->
	<section id="simple" class="main style1">
		<div class="container">
			<div class="inner">
				<ul class="actions special">
					<li><a href="index.html" class="button scrolly">Home</a></li>
					<li><a href="#simple" class="button scrolly">Simple LSTM</a></li>
					<li><a href="#bi" class="button scrolly">Bidirectional LSTM</a></li>
					<li><a href="#hyper" class="button scrolly">Hyperparameters</a></li>
				</ul>
			</div>
			<div class="row gtr-150">
				<div class="col-6 col-12-medium">
					<header class="major">
						<h2>Baseline LSTM<br />
						</h2>
					</header>
					<h3>The Simple Child</h3>
					<p>
						An LSTM is a type of Recurrent Neural Network, which is formed by <strong> adding a short and long term
						memory unit </strong> to an RNN.
						The addition of memory units allows the network to deal with the correlation of time series in
						the short and long term,
						and <strong> store dependencies that it deems important </strong> from earlier epochs of training.
					</p>
					<p>
						LSTMs have cells in the hidden layers of the neural network, which have three gates: input, an
						output, and a forget gate.
						These gates control the flow of information which is needed to predict the output in the
						network.
						The gates that are added to an LSTM cell allow LSTMs to learn long term dependencies, since they
						are able to retain information from multiple previous time-steps.
					</p>
					<p>
						The image on the right is an example of an LSTM cell.
						Input is received from the previous cell and passes through the forget gates, where values
						deemed unimportant are dropped.
						Eventually new values are sent to the next cell through the output gate.
					</p>
					<ul class="actions special">
							<li><a href="#bi" class="button scrolly">Next: Bidirectional LSTM</a></li>
						</ul>
				</div>
				<div class="col-6 col-12-medium imp-medium">
					<span class="image fit"><img src="./images/RHNrZ.jpeg" alt="" /></span>
				</div>
			</div>
	</section>

	<!-- Preliminary Data Analysis -->
	<section id="bi" class="main style2">
		<div class="container">
			<div class="row gtr-150">
				<div class="col-6 col-12-medium imp-medium">
					<span class="image fit"><img src="./images/bi.jpeg" alt="" /></span>
					<span class="image fit"><img src="./images/bi-table.png" alt="" /></span>
				</div>
				<div class="col-6 col-12-medium">
					<header class="major">
						<h2>Bidirectional LSTM<br />
						</h2>
					</header>
					<h3>Going Back and Forth</h3>
					<p>
						A Bidirectional LSTM runs input from both past to future, and future to past.
						This approach <strong> preserves information from the future </strong> and, using two hidden states combined,
						it is is able in any point in time to preserve information.
					</p>
					<p>
						The Bidirectional LSTM fitted to the training data very well, and had the lowest validation loss
						of all the LSTM models.
						Since information is considered from both the past and the future, it may have considered future
						flows which the other models were not aware of yet.
						During the prediction phase, the Bidirectional LSTM had a higher error rate than the Simple and
						Stacked LSTM.
					</p>
					<p>The results show that there is no benefit to using the Bidirectional model for training and
						predicting network traffic on the SANREN,
						as its complexity does not yield an improvement in prediction accuracy compared to the Simple
						LSTM.</p>
						<ul class="actions special">
								<li><a href="#hyper" class="button scrolly">Next: Hyperparameter Tuning</a></li>
							</ul>
				</div>
			</div>
		</div>
	</section>

	<!-- Stacked LSTM -->
	<section id="hyper" class="main style1">
		<div class="container">
			<div class="row gtr-150">
				<div class="col-6 col-12-medium imp-medium">
					<span class="image fit"><img src="./images/SimpleMSEvsEpochs.png" alt="" /></span>
					<span class="image fit"><img src="./images/TrainingvsEpochs.png" alt="" /></span>
				</div>
				<div class="col-6 col-12-medium">
				
					<h2>Hyperparameter Tuning</h2>
						<header class="major">
					</header>
					<h3>It's A Fine Line</h3>
					<p>
						All three of the LSTM models are parameterised functions,
						meaning that there is an optimal combination of parameters
						that will minimise the chosen cost function and produce the
						best performing model [29]. This process is known as hyperparameter tuning. The simple,
						bidirectional and stacked
						LSTM models were fitted over a range of hyperparameters.
						In the context of LSTMs, these parameters are epochs - the
						number of times the data is passed through the model - and
						neurons - the number of memory units in each LSTM layer.
						In this study, each LSTM model was trained using a range
						of epochs from 25 to 150, with 25 epoch intervals, and with
						50 or 100 neurons in each LSTM layer of the model. This
						resulted in 12 permutations of hyperparameter combinations for each model.
					</p>

					<p>The optimisation of the stacked LSTM was done using Mean Average Error, Mean Squared Error and
						Rsquared as assesors of the model's performance on training and validation data. The entire
						optimisation process can be
						seen in Antony's Final Paper, although Figure ? best demonstrates why the stacked LSTM was
						optimised the way it was. The optimal
						hyperparameters for the stacked LSTM model are 100 epochs and 50 neurons.
					</p>
					<ul class="actions special">
						<li><a href="index.html#models" class="button scrolly">Back to Home</a></li>
					</ul>
				</div>
			</div>
		</div>
	</section>

	<!-- Footer -->
	<section id="footer">
		<ul class="icons">
			<li><a href="https://github.com/fle1scha/Deep-Learning-Network-Traffic-Prediction"
					class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
		</ul>
		<ul class="copyright">
			<li>&copy; Justin Myerson and Antony Fleischer</li>
		</ul>
	</section>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>